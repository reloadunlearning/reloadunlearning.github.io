<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Mitigating Privacy Risk via Forget Set-Free Unlearning">
  <meta name="keywords" content="<b>Reload</b>, unlearning, corrective, partially-blind">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Mitigating Privacy Risk via Forget Set-Free Unlearning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="http://projectavi.net/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

<!--      <div class="navbar-item has-dropdown is-hoverable">-->
<!--        <a class="navbar-link">-->
<!--          More Research-->
<!--        </a>-->
<!--        <div class="navbar-dropdown">-->
<!--          <a class="navbar-item" href="https://hypernerf.github.io">-->
<!--            HyperNeRF-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://nerfies.github.io">-->
<!--            Nerfies-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://latentfusion.github.io">-->
<!--            LatentFusion-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://photoshape.github.io">-->
<!--            PhotoShape-->
<!--          </a>-->
<!--        </div>-->
<!--      </div>-->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Mitigating Privacy Risk via Forget Set-Free Unlearning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://projectavi.net/">Aviraj Newatia</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://michaeljohncooper.com/">Michael Cooper</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://www.cs.toronto.edu/~viet/">Viet Nguyen</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.toronto.edu/~rahulgk/">Rahul G. Krishnan</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Toronto,</span>
            <span class="author-block"><sup>2</sup>Vector Institute</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=d3R0TF7w5f"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>ICLR2026 Conference Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=DNlWbTTQYX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>NeurIPS2024 Workshop Paper</span>
                </a>
              </span>
<!--              <span class="link-block">-->
<!--                <a href="https://arxiv.org/abs/2011.12948"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>arXiv</span>-->
<!--                </a>-->
<!--              </span>-->
<!--              &lt;!&ndash; Video Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video </span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
<!--              &lt;!&ndash; Dataset Link. &ndash;&gt;-->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="far fa-images"></i>-->
<!--                  </span>-->
<!--                  <span>Data</span>-->
<!--                  </a>-->
<!--                </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/method_fig_main.png"
           class="interpolation-image"
           alt="main method figure."
           style="width: 85%; margin: 0 auto; display: block; background-color: #ffffff; padding: 15px; border-radius: 5px;"/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf"><b>Reload</b></span> unlearns private data and data corruptions without access to the forget-set.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Training machine learning models requires the storage of large datasets, which often contain sensitive or private data.
            Storing data is associated with a number of potential risks which increase over time, such as database breaches and malicious adversaries.
          </p>
          <p>
            Machine unlearning is the study of methods to efficiently remove the influence of training data subsets from previously-trained models.
            Existing unlearning methods typically require direct access to the "forget set"---the data to be forgotten-and organisations must retain this data for unlearning rather than deleting it immediately upon request, increasing risks associated with the forget set.
          </p>
          <p>
            We introduce <i>partially-blind unlearning</i>---utilizing auxiliary information to unlearn without explicit access to the forget set.
            We also propose a practical framework <b>Reload</b>, a partially-blind method based on gradient optimization and structured weight sparsification to operationalize partially-blind unlearning.
          </p>
          <p>
            We show that <b>Reload</b> efficiently unlearns, approximating models retrained from scratch, and outperforms several forget set-dependent approaches. On language models, <b>Reload</b> unlearns entities using <0.025% of the retain set and <7% of model weights in <8 minutes on Llama2-7B.
            In the corrective case, <b>Reload</b> achieves unlearning even when only 10% of corrupted data is identified.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <div class="content">
          <h2 class="title is-3">Privacy Risk</h2>
          <p>
            In many facets of modern life, individuals consent for institutions to collect and use their personal data.
            The act of collecting and storing a user's data poses inherent risk to the user.
          </p>
          <img src="./static/images/privacy_risk.png"
               class="interpolation-image"
               alt="privacy risk figure."
               style="width: 50%; margin: 0 auto; display: block; background-color: #ffffff; padding: 15px; border-radius: 5px;"/>
          <p>
            Informally, modern machine learning systems expose the user to two types of risk: <i>dataset risk</i> represents the user risk associated with an institution storing a user's data, while <i>model risk</i> represents the additional risk to the user when their data is used to train a machine learning model.
          </p>
          <p>
            Conventional unlearning algorithms admit a cumulative user risk totalling the sum of the <span style="color: #D5E6DE">green</span>, <span style="color: #BEC1D3">blue</span>, and <span style="color: #EFA5A5">red</span> regions. By allowing user data to be deleted immediately once a request for deletion is made, <b>Reload</b> eliminates the user risk associated with the <span style="color: #EFA5A5">red</span> region.
          </p>

        </div>

        <h2 class="title is-3">Reload Unlearning</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              <b>Reload</b>, combines insights from three families of unlearning algorithms - gradient-based, structured sparsity-based, and finetuning-based algorithms. An organisation using <b>Reload</b> would be able to immediately delete user data once a request for deletion is made without inhibiting downstream unlearning.
            </p>
            <img src="./static/images/method_fig_main.png"
                 class="interpolation-image"
                 alt="main method figure."
                 style="width: 70%; margin: 0 auto; display: block; background-color: #ffffff; padding: 15px; border-radius: 5px;"/>
            <p>
              <strong>1. Ascent:</strong> <strong><b>Reload</b></strong> performs a single gradient ascent step to move the model weights away from convergence on the forget set using cached gradients.
            </p>

            <p>
              <strong>2. Re-initialisation:</strong> It computes a knowledge value for each weight to identify its reliance on the forget data, selectively re-initializing the weights below a specified quantile threshold to yield new parameters.
            </p>

            <p>
              <strong>3. Finetuning:</strong> Finally, the modified model is fine-tuned to convergence on the retain set via gradient-based optimization to recover overall performance.
            </p>

          </div>

        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h2 class="title is-3">Empirical Results</h2>

        <h3 class="title is-4">Methodological Introspection</h3>
        <div class="content has-text-justified">
          <p>
            In observing model representations during <b>Reload</b>, we can visualise each step of the process of unlearning the class "8" from a ResNet-18 model trained on the SVHN dataset, showing the contributon of each component of <b>Reload</b> to the unlearning process.
          </p>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/images/convnet_introspect_predicted.png"
                 class="interpolation-image"
                 alt="introspection figure."
                 style="width: 80%; margin: 0 auto; display: block; background-color: #ffffff; padding: 15px; border-radius: 5px;"/>
          </div>
        </div>

        <!-- Interpolating. -->
        <h3 class="title is-4">Classical Unlearning Results</h3>
        <div class="content has-text-justified">
          <p>
            Using <b>Reload</b> a practitioner can unlearn random and correlated data from an image classification model and unlearn entities from language models without accessing that data. This enables private, secure, and efficient unlearning while strengthening guarantees provided by legislature such as GDPR and the "right to be forgotten". Full results are in the paper.
          </p>
        </div>

        <div class="columns is-vcentered interpolation-panel">
          <img src="./static/images/reload_table.jpg"
               class="interpolation-image"
               alt="results table."/>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Corrective Unlearning</h3>
        <div class="content has-text-justified">
          <p>
            <b>Reload</b> can remove data corruptions even when only 10% of the corrupted data is identified, without using that data. This means that using <b>Reload</b>, practitioners can mitigate the dangers presented by adversarial, incorrect, or outdated data without needing to retrain a model from scratch or be worried about the influence of outdated data.
          </p>
          <br/>
          <div class="columns is-vcentered interpolation-panel">
            <img src="./static/images/corrective_expr.png"
                 class="interpolation-image"
                 alt="results table."
                 style="width: 67%; margin: 0 auto; display: block; background-color: #ffffff; padding: 15px; border-radius: 5px;"/>
          </div>
        </div>
        <div class="content has-text-centered">
<!--          <video id="replay-video"-->
<!--                 controls-->
<!--                 muted-->
<!--                 p<b>Reload</b>-->
<!--                 playsinline-->
<!--                 width="75%">-->
<!--            <source src="./static/videos/replay.mp4"-->
<!--                    type="video/mp4">-->
<!--          </video>-->
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{
  newatia2026mitigating,
  title={Mitigating Privacy Risk via Forget Set-Free Unlearning},
  author={Aviraj Newatia and Michael Cooper and Viet Nguyen and Rahul G Krishnan},
  booktitle={The Fourteenth International Conference on Learning Representations},
  year={2026},
  url={https://openreview.net/forum?id=d3R0TF7w5f}
  }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/projectavi" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This page is built from the Nerfies website <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
